#!/usr/bin/env python3

"""
Generate measurements with data/ground truth for experiment 34
"""

import os
import json
import pyshark
import sqlite3
import glob
from statistics import median

from datetime import datetime
from zipfile import ZipFile
from scipy.spatial import distance

# NOTE Taken from the following code snippet
# public static int advertiseTxPowerO() {
#     return -15; // AdvertisingSetParameters.TX_POWER_LOW
# }
TX_POWER_GAEN_15 = -15


MODEL_RX_TX_COMPENSATION = { # date: 200813
# Official Google calibration of 20/08/13
    'SM-G973F': [5.0, -24.0],
    'SM-G970F': [5.0, -24.0],
    'SM-A405FN': [5.0, -24.0],
    'SM-A515F': [5.0, -24.0],
    'SM-A908B': [5.0, -24.0],
    'SM-G981B/DS': [5.0, -24.0] # SM-G981B in table
}


R = {
    'id': 'db_id',
    'rid': 'receiver_id',
    'rmo': 'receiver_model',
    'tid': 'transmitter_id',
    'tmo': 'transmitter_model',
    'ts' : 'timestamp',
    'rssi': 'rssi',
    'txp' : 'tx_power',
    'gtd' : 'ground_truth_distance',
}

MODEL_MAP_EPFL_ANDROID = [
     0,
    'SM-G973F',
    'SM-G973F',
    'SM-G970F',
    'SM-G970F',
    'SM-A405FN',
    'SM-G981B/DS',
    'SM-G973F',
    'SM-A908B',
    'SM-G973F',
    'SM-A515F',
    'SM-G973F',
    'SM-G973F',
    'SM-A405FN',
    'SM-A405FN',
    'SM-A405FN',
    'SM-A405FN',
    'SM-A405FN',
    'SM-A405FN',
    'SM-A405FN',
    'SM-A405FN',
    'SM-A405FN',
    'SM-A405FN',
    'GA01187-DE',
    'GA01187-DE',
]
assert len(MODEL_MAP_EPFL_ANDROID) == 25

AND_ID = [1,2,3,4,6,7,8,9,10,11,12,13,14,15,17,18,19,20]
AND_HCI_ID = [1,2,3,4,6,7,8,9,11,12]
AND_VS_ID = [10,13,14,15,17,18,19,20]
assert len(AND_ID) == len(AND_HCI_ID)+len(AND_VS_ID)

BTSNOOP_OFFSET = 2 * 60 * 60


def create_db(path, filename, rm_old=False):
    """Create result table in results.sqlite

    :path: path to the new table

    """
    if rm_old and os.path.exists('{}.sqlite'.format(filename)):
        os.remove('{}{}.sqlite'.format(path, filename))
        print('Removed old {}{}.sqlite'.format(path, filename))

    try:
        conn = sqlite3.connect('{}{}.sqlite'.format(path, filename))
        conn.execute('''CREATE TABLE results(
            {}    INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,
            {}    TEXT NOT NULL,
            {}    TEXT NOT NULL,
            {}    TEXT NOT NULL,
            {}    TEXT NOT NULL,
            {}    REAL NOT NULL,
            {}    INTEGER NOT NULL,
            {}    INTEGER,
            {}    REAL
        )'''.format(R['id'], R['rid'], R['rmo'], R['tid'], R['tmo'],
            R['ts'], R['rssi'], R['txp'], R['gtd']))

    except sqlite3.OperationalError as e:
        print(e, 'if you want a new one: rm {}{}.sqlite'.format(path, filename))

    conn.close()


def put_results(path, results, filename=None):
    """Put results into results.sqlite

    :path: path to the db folder, e.g. exp01/trial01/
    :results: list of tuple generated by get_results

    """

    if filename == None:
        conn = sqlite3.connect('{}results.sqlite'.format(path))
    else:
        conn = sqlite3.connect('{}{}.sqlite'.format(path, filename))

    c = conn.cursor()
    iq = """INSERT INTO results ({}, {}, {}, {}, {}, {}, {}, {})
        VALUES (?, ?, ?, ?, ?, ?, ?, ?);""".format(
            R['ts'], R['tid'], R['tmo'], R['txp'], R['rssi'],
            R['rid'], R['rmo'], R['gtd'])
    c.executemany(iq, results)
    conn.commit()

    conn.close()


def parse_vscmds(path, rx, ti):
    """Use to parse HCI LE Meta Adv Report

    """
    ephids = []

    rx_str = str(rx)
    if len(rx_str) == 1:
        rx_str = '0'+rx_str

    vs_rpi = 'bthci_cmd  and (frame contains 03:03:6f:fd:17:16:6f:fd)'
    pkts = pyshark.FileCapture(path+rx_str+'.btsnoop', display_filter=vs_rpi)
    raw_pkts = pyshark.FileCapture(path+rx_str+'.btsnoop', display_filter=vs_rpi,
            use_json=True, include_raw=True)

    i = 0
    for pkt in pkts:
        # NOTE: correct btsnoop timestamp
        pkt_datetime = datetime.fromtimestamp(pkt.sniff_time.timestamp()-BTSNOOP_OFFSET)
        if pkt_datetime < ti[0] or pkt_datetime > ti[1]:
            i += 1
            continue
        else:
            # NOTE: remove first 14 bytes and last 4 bytes
            raw_ephid = raw_pkts[i].get_raw_packet().hex()[28:-8]
            assert len(raw_ephid) == 40

            ephid = ''
            for j in range(0,40,2):
                ephid += raw_ephid[j:j+2]
                if j != 38:
                    ephid += ':'

            if ephid not in ephids:
                ephids.append(ephid)
            i += 1

    print('')
    print('rx', rx)
    print('ephids', ephids)
    print('len(ephids)', len(ephids))
    print('')

    pkts.close()
    raw_pkts.close()

    with open(path+rx_str+'.ephids', "w") as wf:
        for ephid in ephids:
            wf.write(ephid+'\n')


def parse_hcicmds(path, rx, ti):
    """Use to parse HCI Set Ext Adv Data

    """
    ephids = []


    rx_str = str(rx)
    if len(rx_str) == 1:
        rx_str = '0'+rx_str

    with open(path+rx_str+'.json', "r") as rf:
        pkts = json.load(rf)

    for pkt in pkts:
        # NOTE: exclude packets outside the experimental range
        pkt_timestamp = str(float(pkt['_source']['layers']['frame']['frame.time_epoch'])-BTSNOOP_OFFSET)
        # NOTE: timestamp from the btsnoop are 2 hours ahead
        pkt_datetime = datetime.fromtimestamp(float(pkt_timestamp))
        if pkt_datetime < ti[0] or pkt_datetime > ti[1]:
            continue
        # print(pkt_datetime)

        pkt_source = pkt['_source']['layers']['bluetooth']['bluetooth.src_str']
        # NOTE:HCI cmd
        if pkt_source == 'host':
            pkt_ads = pkt['_source']['layers']['bthci_cmd']['btcommon.eir_ad.advertising_data']
            pkt_ephid = pkt_ads['btcommon.eir_ad.entry']['btcommon.eir_ad.entry.service_data']
            # print(pkt_ephid)
            if pkt_ephid not in ephids:
                ephids.append(pkt_ephid)
    print('')
    print('rx', rx)
    print('ephids', ephids)
    print('len(ephids)', len(ephids))
    print('')

    with open(path+rx_str+'.ephids', "w") as wf:
        for ephid in ephids:
            wf.write(ephid+'\n')


def get_ephids(path):
    """Returns a dict of ephids indexed by filename
    """

    ephids = {}

    for i in AND_ID:
        i_str = str(i)
        if len(i_str) == 1:
            i_str = '0'+i_str
        ephids[i_str] = []
        with open(path+i_str+'.ephids', "r") as rf:
            for line in rf:
                if line == '\n':
                    continue
                ephid = line.strip()
                assert len(ephid) == 59   # 20 bytes like aa:bb:..:ff
                ephids[i_str].append(ephid)

    return ephids


def parse_hcievents(path, rx, ti):
    """Use to parse HCI LE Meta Adv Report

    """
    results = []

    ephids = get_ephids(path)
    rx_str = str(rx)
    if len(rx_str) == 1:
        rx_str = '0'+rx_str

    with open(path+rx_str+'.json', "r") as rf:
        pkts = json.load(rf)

    for pkt in pkts:
        # NOTE: exclude packets outside the experimental range
        pkt_timestamp = str(float(pkt['_source']['layers']['frame']['frame.time_epoch'])-BTSNOOP_OFFSET)
        pkt_datetime =  datetime.fromtimestamp(round(float(pkt_timestamp)))
        if pkt_datetime < ti[0] or pkt_datetime > ti[1]:
            continue
        # print(pkt_datetime)

        pkt_source = pkt['_source']['layers']['bluetooth']['bluetooth.src_str']
        # NOTE:HCI cmd
        if pkt_source == 'controller':
            pkt_rssi = pkt['_source']['layers']['bthci_evt']['bthci_evt.rssi']
            # print('rssi', pkt_rssi)
            pkt_ads = pkt['_source']['layers']['bthci_evt']['btcommon.eir_ad.advertising_data']
            pkt_ephid = pkt_ads['btcommon.eir_ad.entry']['btcommon.eir_ad.entry.service_data']
            # print('ephid', pkt_ephid)

            for key, value in ephids.items():
                if pkt_ephid in ephids[key]:
                    # print('tx', key, pkt_ephid)
                    tx = int(key)
                    gtd = None
                    txpower = 0
                    tmo = MODEL_MAP_EPFL_ANDROID[tx]
                    rmo = MODEL_MAP_EPFL_ANDROID[rx]
                    results.append((pkt_timestamp, tx, tmo, txpower, pkt_rssi, rx, rmo, gtd))

    return results


def get_cv_json(path, filename):
    with ZipFile(path+filename) as myzip:
        with myzip.open(filename[:-4]+'/'+filename[2:-4]+'.json') as myfile:
            data = "".join(map(chr, myfile.read()))
            json_obj = json.loads(data)

    # NOTE: trasform datetime list of str in list of datetime objs
    for i in range(0, len(json_obj['datetime'])):
        json_obj['datetime'][i] = datetime.strptime(json_obj['datetime'][i],
                '%Y-%m-%d %H:%M:%S.%f')
    return json_obj


def check_cv_json_obj(json_obj):
    """Check that cv json format is correct."""
    for key in json_obj.keys():
        assert len(json_obj[key]) == len(json_obj['datetime'])


def nearest(datetimes, search_datetime):
    value = min(datetimes, key=lambda x: abs(x - search_datetime))
    key = datetimes.index(value)

    return key, value

def add_gtd(results, json_obj):
    json_obj_keys = json_obj.keys()
    check_cv_json_obj(json_obj)

    for i in range(0, len(results)):
        if i % 2000 == 0:
            print(i)
        rx = results[i][5]
        if rx not in json_obj_keys:
            continue
        tx = results[i][1]
        if tx not in json_obj_keys:
            continue
        dt = datetime.fromtimestamp(results[i][0])
        ndt_key, ndt_value = nearest(json_obj['datetime'], dt)

        rx_coord = json_obj[rx][ndt_key]
        if type(rx_coord) != list:
            continue
        tx_coord = json_obj[tx][ndt_key]
        if type(tx_coord) != list:
            continue
        # NOTE gtd is in m
        gtd = float(distance.euclidean(rx_coord[0:2], tx_coord[0:2]))
        print('i: {}, rx: {}, tx: {}, gtd: {}'.format(i, rx, tx, gtd))
        results[i][7] = gtd

    return results


def get_sqlite_results(path, order_by=None):
    """Return a list of lists."""
    results = []

    # NOTE: result format [pkt_timestamp, tx, txpower, pkt_rssi, rx, gtd]
    query = "SELECT {}, {}, {}, {}, {}, {}, {}, {} from results".format(
            R['ts'], R['tid'], R['rmo'], R['txp'], R['rssi'], R['rid'], R['rmo'], R['gtd'])

    if order_by != None:
        query += ' ORDER BY {} ASC'.format(order_by)

    conn = sqlite3.connect(path)
    cursor = conn.cursor()
    cursor.execute(query)
    tuple_results = cursor.fetchall()
    for t in tuple_results:
        results.append(list(t))

    return results


def get_en_jsons(path):
    jsons = {}

    paths = glob.glob(path+'*.json')
    for path in paths:
        with open(path) as fp:
            filename = path[15:]
            jsons[filename] = json.load(fp)
    assert len(jsons) == len(paths)

    return jsons


def get_en_results(jsons, aggregation='min'):

    results = []

    # NOTE: footer depends on device id
    en_name = 'experiment_epfl-soldiers_'

    for key, val in jsons.items():
        rid = key[0:2]
        rmo = MODEL_MAP_EPFL_ANDROID[int(rid)]
        tid = key[8:10]
        tmo = MODEL_MAP_EPFL_ANDROID[int(tid)]

        for window in val[en_name+rid]:
            # NOTE: ts is not precise, backend provides only the date
            ts = window['dateMillisSinceEpoch']
            # FIXME: calibrate according to the model name
            txp = 1.0
            # NOTE: we don't know the gtd
            gtd = None
            for scan_instance in window['scanInstances']:
                # NOTE: attenuation in DB
                att_min = scan_instance['minAttenuationDb']
                att_typical = scan_instance['typicalAttenuationDb']
                if aggregation == 'min':
                    rssi = att_min
                else:
                    rssi = att_typical
                results.append([ts, tid, tmo, txp, rssi, rid, rmo, gtd])

    return results


def rssi_to_attenuation(tmo, rmo, tx_power, rssi, compensation=MODEL_RX_TX_COMPENSATION):
    """Return attenuation.

    Adjust is used as an uncalibrated txpower value

    More info at:

        https://developers.google.com/android/exposure-notifications/ble-attenuation-overview#attenuations_as_distance_proxy
    """
    attenuation = 0
    txadjust = compensation[rmo][1]
    rxadjust = compensation[tmo][0]
    attenuation = int(txadjust - (rssi + rxadjust))
    assert attenuation > 0 and type(attenuation) == int
    return attenuation


def gen_en_from_sqlite(path, time_interval, exp_win_mins, keep_raw=False):
    """
    Returns a dict in the form:

    rx_check_tx = [ win1, ..., winn ]

        win = [ scan_ins1, ..., scan_insn ]

            scan_ins = [ rx1_checks, ..., rxn_checks ]

                rx_checks = [ [ tx1_min, tx1_avg ]_, ..., [txn_min, txn_avg ]  ]

    Note that this function assumes that the packet log is already in GAEN
    format, i.e., that the packet trace/pcap was created from a GAEN packet
    dump. I.e., this assumes that the scan windows are already filtered based
    on wakeup times from the GAEN framework.
    """
    assert type(path) == str
    assert type(exp_win_mins) == int

    results = []
    query =  "SELECT {}, {}, {}, {}, {}, {}, {}, {} FROM results".format(
        R['ts'], R['tid'], R['txp'], R['rssi'], R['rid'],
        R['tmo'], R['rmo'], R['gtd'])
    query += " WHERE timestamp >= ? AND timestamp <= ?"
    query += " ORDER BY timestamp ASC"
    query_values = (time_interval[0], time_interval[1])
    conn = sqlite3.connect(path)
    cursor = conn.cursor()
    cursor.execute(query, query_values)
    tuple_results = cursor.fetchall()
    for t in tuple_results:
        results.append(list(t))

    rx_check_tx = {}
    # NOTE: windows and scan_instance count from 0
    windows = 0
    scan_ins = 0
    ts_start = time_interval[0]
    ts_last = results[0][0]
    ts_start_scan_ins = time_interval[0]
    ts_stop = ts_start + (60.0 * exp_win_mins)

    for i in range(0, len(results)):
        ts_current = results[i][0]
        tx = results[i][1]
        rx = results[i][4]
        rssi = results[i][3]
        tx_power = results[i][2]
        tmo = results[i][5]
        rmo = results[i][6]
        gtd = results[i][7]

        # NOTE: next scan instance
        # ASSUMPTION: if last beacon was >6s ago, assume we're in next window
        if ts_current - ts_last > 6.0 and ts_current <= ts_stop:
            rx_check_tx = process_scan_instance(rx_check_tx, windows, scan_ins)
            ts_start_scan_ins = ts_current
            scan_ins += 1

        # NOTE: next window
        elif ts_current > ts_stop:

            # NOTE: process last scan instance in the window
            rx_check_tx = process_scan_instance(rx_check_tx, windows, scan_ins)

            ts_start = ts_stop
            ts_stop = ts_start + (60 * exp_win_mins)
            ts_start_scan_ins = ts_start
            windows += 1
            scan_ins = 0

        ts_last = ts_current

        # NOTE: create needed rx_check_tx keys
        if windows not in rx_check_tx:
            rx_check_tx[windows] = {}
        if scan_ins not in rx_check_tx[windows]:
            rx_check_tx[windows][scan_ins] = {}
        if rx not in rx_check_tx[windows][scan_ins]:
            rx_check_tx[windows][scan_ins][rx] = {}
        if tx not in rx_check_tx[windows][scan_ins][rx]:
            rx_check_tx[windows][scan_ins][rx][tx] = {}
            rx_check_tx[windows][scan_ins][rx][tx]['atts'] = []
            rx_check_tx[windows][scan_ins][rx][tx]['ts'] = []
            rx_check_tx[windows][scan_ins][rx][tx]['gtds'] = []
            rx_check_tx[windows][scan_ins][rx][tx]['min_att'] = None
            rx_check_tx[windows][scan_ins][rx][tx]['avg_att'] = None
            rx_check_tx[windows][scan_ins][rx][tx]['min_gtd'] = None
            rx_check_tx[windows][scan_ins][rx][tx]['avg_gtd'] = None

        attenuation = rssi_to_attenuation(tmo, rmo, tx_power, rssi)
        rx_check_tx[windows][scan_ins][rx][tx]['atts'].append(attenuation)
        rx_check_tx[windows][scan_ins][rx][tx]['ts'].append(round(ts_current, 2))
        if gtd != None:
            rx_check_tx[windows][scan_ins][rx][tx]['gtds'].append(gtd)


    # NOTE: process last scan instance
    rx_check_tx = process_scan_instance(rx_check_tx, windows, scan_ins)

    with open(path[:-6]+'json', 'w') as outfile:
            json.dump(rx_check_tx, outfile)

    return rx_check_tx


def gen_en_from_backend(path, time_interval, windows, keep_raw=False):
    """
    Generate rx_check_tx using variable scan instances inside 30 min windows

    Returns a dict in the form:

    rx_check_tx = [ win1, ..., winn ]

        win = [ scan_ins1, ..., scan_insn ]

            scan_ins = [ rx1_checks, ..., rxn_checks ]

                rx_checks = [ [ tx1_min, tx1_avg ]_, ..., [txn_min, txn_avg ]  ]
    """
    assert type(path) == str
    assert type(windows) == list

    # [[3, 5, 5, 7, 5, 5], [...], [...]]

    results = []
    query =  "SELECT {}, {}, {}, {}, {}, {}, {}, {} FROM results".format(
        R['ts'], R['tid'], R['txp'], R['rssi'], R['rid'],
        R['tmo'], R['rmo'], R['gtd'])
    query += " WHERE timestamp >= ? AND timestamp <= ?"
    query += " ORDER BY timestamp ASC"
    query_values = (time_interval[0], time_interval[1])
    conn = sqlite3.connect(path)
    cursor = conn.cursor()
    cursor.execute(query, query_values)
    tuple_results = cursor.fetchall()
    for t in tuple_results:
        results.append(list(t))

    rx_check_tx = {}
    # NOTE: windows and scan_instance count from 0
    windows = 0
    scan_ins = 0
    ts_start = time_interval[0]
    ts_start_scan_ins = time_interval[0]
    ts_next_scan_ins = ts_start + (60.0 * scan_win_mins)
    ts_stop = ts_start + (60.0 * exp_win_mins)

    for i in range(0, len(results)):
        ts_current = results[i][0]
        tx = results[i][1]
        rx = results[i][4]
        rssi = results[i][3]
        tx_power = results[i][2]
        tmo = results[i][5]
        rmo = results[i][6]
        gtd = results[i][7]

        # NOTE: next scan instance
        if ts_current > ts_next_scan_ins and ts_current <= ts_stop:
            rx_check_tx = process_scan_instance(rx_check_tx, windows, scan_ins)
            ts_start_scan_ins = ts_next_scan_ins
            ts_next_scan_ins = ts_start_scan_ins + (60 * scan_win_mins)
            scan_ins += 1

        # NOTE: next window
        elif ts_current > ts_stop:

            # NOTE: process last scan instance in the window
            rx_check_tx = process_scan_instance(rx_check_tx, windows, scan_ins)

            ts_start = ts_stop
            ts_stop = ts_start + (60 * exp_win_mins)
            ts_start_scan_ins = ts_start
            ts_next_scan_ins = ts_start_scan_ins + (60 * scan_win_mins)
            assert scan_ins + 1 == exp_win_mins // scan_win_mins
            windows += 1
            scan_ins = 0

        # NOTE: create needed rx_check_tx keys
        if windows not in rx_check_tx:
            rx_check_tx[windows] = {}
        if scan_ins not in rx_check_tx[windows]:
            rx_check_tx[windows][scan_ins] = {}
        if rx not in rx_check_tx[windows][scan_ins]:
            rx_check_tx[windows][scan_ins][rx] = {}
        if tx not in rx_check_tx[windows][scan_ins][rx]:
            rx_check_tx[windows][scan_ins][rx][tx] = {}
            rx_check_tx[windows][scan_ins][rx][tx]['atts'] = []
            rx_check_tx[windows][scan_ins][rx][tx]['ts'] = []
            rx_check_tx[windows][scan_ins][rx][tx]['gtds'] = []
            rx_check_tx[windows][scan_ins][rx][tx]['min_att'] = None
            rx_check_tx[windows][scan_ins][rx][tx]['avg_att'] = None
            rx_check_tx[windows][scan_ins][rx][tx]['min_gtd'] = None
            rx_check_tx[windows][scan_ins][rx][tx]['avg_gtd'] = None

        attenuation = rssi_to_attenuation(tmo, rmo, tx_power, rssi)
        rx_check_tx[windows][scan_ins][rx][tx]['atts'].append(attenuation)
        rx_check_tx[windows][scan_ins][rx][tx]['ts'].append(round(ts_current, 2))
        if gtd != None:
            rx_check_tx[windows][scan_ins][rx][tx]['gtds'].append(gtd)


    # NOTE: process last scan instance
    rx_check_tx = process_scan_instance(rx_check_tx, windows, scan_ins)

    with open(path[:-6]+'json', 'w') as outfile:
            json.dump(rx_check_tx, outfile)

    return rx_check_tx


def process_scan_instance(rx_check_tx, windows, scan_ins, keep_raw=False):
    # NOTE: compute min and avg
    for rx in rx_check_tx[windows][scan_ins]:
        for tx in rx_check_tx[windows][scan_ins][rx]:

            # NOTE: keep min and average attenuation
            min_att = min(rx_check_tx[windows][scan_ins][rx][tx]['atts'])
            avg_att = avg_int(rx_check_tx[windows][scan_ins][rx][tx]['atts'])
            rx_check_tx[windows][scan_ins][rx][tx]['min_att'] = min_att
            rx_check_tx[windows][scan_ins][rx][tx]['avg_att'] = avg_att

            # NOTE: set min and avg gtd to None if there are values
            if len(rx_check_tx[windows][scan_ins][rx][tx]['gtds']) == 0:
                min_gtd = None
                avg_gtd = None
            else:
                min_gtd = round(min(rx_check_tx[windows][scan_ins][rx][tx]['gtds']), 2)
                avg_gtd = avg(rx_check_tx[windows][scan_ins][rx][tx]['gtds'])
                #avg_gtd = median(rx_check_tx[windows][scan_ins][rx][tx]['gtds'])
            rx_check_tx[windows][scan_ins][rx][tx]['min_gtd'] = min_gtd
            rx_check_tx[windows][scan_ins][rx][tx]['avg_gtd'] = avg_gtd

            # NOTE: keep only start and end timestamp of the window
            rx_check_tx[windows][scan_ins][rx][tx]['ts'] = [
                rx_check_tx[windows][scan_ins][rx][tx]['ts'][0],
                rx_check_tx[windows][scan_ins][rx][tx]['ts'][-1],
            ]
            if not keep_raw:
                del(rx_check_tx[windows][scan_ins][rx][tx]['atts'])
                del(rx_check_tx[windows][scan_ins][rx][tx]['gtds'])

    return rx_check_tx


def avg(_list, dec_digits=2):
    """Return list's floating-point average."""
    assert type(_list) == list
    return round(sum(_list) / len(_list), dec_digits)


def avg_int(_list):
    """Return list's integer average."""
    assert type(_list) == list
    return int(sum(_list) / len(_list))


def rx_from_en(rx, tx, en):
    assert type(rx) == str
    assert type(tx) == str
    assert type(en) == dict
    assert len(en) > 0

    i = 0
    temp  = []
    for win in en:
        temp.append([])
        for si in en[win]:
            if rx in en[win][si].keys():
                if tx in en[win][si][rx].keys():
                    temp[i].append(en[win][si][rx][tx])
        i += 1

    # NOTE: remove empty windows
    rx_checks_tx  = []
    for win in temp:
        if len(win) != 0:
            rx_checks_tx.append(win)

    return rx_checks_tx


def score_scan_instance(t1, t2, si, att_stat, gtd_stat, cd):
    """Return score for scan instance that is also encoding T/F P/N

    Score encoding:
        +1.0 and 0.5 are true positive (good alarm)
         0.0 is false negative (missed alarm)
        -1.0 is false positive (false alarm)
        -2.0 is true negative (no alarm)
    """
    assert type(t1) == int and type(t2) == int and t1 < t2
    assert type(si) == dict
    assert type(cd) == float
    assert att_stat == 'min_att' or att_stat == 'avg_att'
    assert gtd_stat == 'min_gtd' or gtd_stat == 'avg_gtd'

    score = 0.0

    if si[gtd_stat] < cd:
        # NOTE: true positive
        if si[att_stat] <= t1:
            score = 1.0
        # NOTE: true positive
        elif si[att_stat] <= t2:
            score = 0.5
        # NOTE: false negative (missed alarm)
        else:
            score = 0.0
    else:
        # NOTE: false positive (false alarm)
        if si[att_stat] <= t1:
            score = -1.0
        elif si[att_stat] <= t2:
            score = -1.0
        # NOTE: true negative
        else:
            score = -2.0
    return score


def atts_from_backend(path, att_stat):
    """Returns a list of windows of aggrefated attenuations (e.g. min, avg)"""
    assert type(path) == str
    assert att_stat == 'min_att' or att_stat == 'avg_att'

    atts = []

    if att_stat == 'min_att':
        att_key = 'minAttenuationDb'
    elif att_stat == 'avg_att':
        att_key = 'typicalAttenuationDb'

    with open(path, "r") as rf:
        raw = json.load(rf)

    i = 0
    windows = raw[list(raw.keys())[0]]
    for window in windows:
        atts.append([])
        # print('window', window)
        for si in window['scanInstances']:
            # print('si', si)
            atts[i].append(si[att_key])
        i += 1
    assert len(atts) == len(windows)

    return atts


def atts_from_rx(rx, att_stat):
    """Returns a list of windows of aggrefated attenuations (e.g. min, avg)"""
    assert type(rx) == list
    assert att_stat == 'min_att' or att_stat == 'avg_att'

    atts = []

    att_key = att_stat
    i = 0
    for window in rx:
        atts.append([])
        for si in window:
            atts[i].append(si[att_key])
        i += 1
    assert len(atts) == len(rx)

    return atts


def get_windows_from_backend(path):
    """
    Each backend file is rx-checktx.json and contains
    a list of UNORDERED exposure windows. Each exposure window should be
    30 min and contains a list of TIME ORDERED scan instances. Each scan
    instance typically last 120, 240, or 300 sec (2, 4, 5 mins). Sometimes a
    list of scan instances is not covering 30 minutes.
    """
    assert type(path) == str

    windows = []

    with open(path, "r") as rf:
        raw = json.load(rf)

    windows = raw[list(raw.keys())[0]]
    seconds = 0.0
    for window in windows:
        for si in window['scanInstances']:
            seconds += si['secondsSinceLastScan']

    # TODO: assert sum of scan instances min is 30
    # TODO: assert seconds is less than 24 hours

    return seconds


def check_backend(rx, tx, en, backend_path, att_stat):
    assert type(rx) == str and type(tx) == str
    assert type(en) == dict
    assert att_stat == 'min_att' or att_stat == 'avg_att'

    matches = []

    rx_check_tx = rx_from_en(rx, tx, en)
    atts = atts_from_rx(rx_check_tx, att_stat)
    atts_be = atts_from_backend(backend_path, att_stat)
    for att_win in atts:
        if att_win in atts_be:
            matches.append(att_win)

    return matches


if __name__ == "__main__":

    # NOTE: window size
    exp_win_mins = 30

    # NOTE: swiss covid thresholds
    SWISS_COVID_THR1_V1 = 50  # dB
    SWISS_COVID_THR2_V1 = 55  # dB
    SWISS_COVID_THR1_V2 = 53  # dB
    SWISS_COVID_THR2_V2 = 60  # dB
    SWISS_COVID_THR1 = 55  # dB
    SWISS_COVID_THR2 = 63  # dB
    SWISS_COVID_CD = 1.5  # m in a scan window

    # NOTE: paths
    cv_path = 'computer-vision/'
    be_path = 'backend-checks/'

    # NOTE: timestamps
    start_lunch = datetime.strptime("2020-07-30 14:49:15.856667",'%Y-%m-%d %H:%M:%S.%f')
    stop_lunch = datetime.strptime( "2020-07-30 15:20:32.378899",'%Y-%m-%d %H:%M:%S.%f')
    start_train = datetime.strptime("2020-07-30 15:28:11.136228",'%Y-%m-%d %H:%M:%S.%f')
    stop_train = datetime.strptime( "2020-07-30 15:59:06.807749",'%Y-%m-%d %H:%M:%S.%f')
    start_office = datetime.strptime("2020-07-30 16:05:17.992095",'%Y-%m-%d %H:%M:%S.%f')
    stop_office = datetime.strptime( "2020-07-30 16:34:12.729496",'%Y-%m-%d %H:%M:%S.%f')
    start_queue = datetime.strptime("2020-07-30 16:43:31.570236",'%Y-%m-%d %H:%M:%S.%f')
    stop_queue = datetime.strptime( "2020-07-30 17:05:08.442719",'%Y-%m-%d %H:%M:%S.%f')
    start_party = datetime.strptime("2020-07-30 17:08:16.140814",'%Y-%m-%d %H:%M:%S.%f')
    stop_party = datetime.strptime( "2020-07-30 17:28:54.631308",'%Y-%m-%d %H:%M:%S.%f')
    start_movement = datetime.strptime("2020-07-30 17:28:54.673010",'%Y-%m-%d %H:%M:%S.%f')
    stop_movement = datetime.strptime( "2020-07-30 17:40:35.215176",'%Y-%m-%d %H:%M:%S.%f')


    # NOTE: generate json from backend
    # ti = [ start_lunch.timestamp(), stop_lunch.timestamp()]
    # seconds = get_windows_from_backend(be_path+'01-check10.json')
    # seconds2 = get_windows_from_backend(be_path+'01-check02.json')
    # seconds3 = get_windows_from_backend(be_path+'02-check10.json')
    # lunch = gen_en_from_sqlite('scenario01-lunch.sqlite', ti, windows)

    # NOTE: match against the backend
    # lunch_match_01_min = {}
    # lunch_match_01_avg = {}
    # train_match_01_min = {}
    # train_match_01_avg = {}
    # office_match_01_min = {}
    # office_match_01_avg = {}
    # queue_match_01_min = {}
    # queue_match_01_avg = {}
    # party_match_01_min = {}
    # party_match_01_avg = {}
    # movement_match_01_min = {}
    # movement_match_01_avg = {}
    # ID1_CHECK = [2,3,6,7,8,9,10,11,12,13,14,15,17,18,19,20]
    # for i in ID1_CHECK:
    #     i_str = str(i)
    #     if len(i_str) == 1:
    #         i_str = '0'+i_str
    #     filename = '01-check{}.json'.format(i_str)

    #     min_matches = check_backend('1', str(i), lunch, be_path+filename, 'min_att')
    #     lunch_match_01_min['01-check'+i_str] = []
    #     lunch_match_01_min['01-check'+i_str].append(min_matches)
    #     avg_matches = check_backend('1', str(i), lunch, be_path+filename, 'avg_att')
    #     lunch_match_01_avg['01-check'+i_str] = []
    #     lunch_match_01_avg['01-check'+i_str].append(avg_matches)

    #     min_matches = check_backend('1', str(i), train, be_path+filename, 'min_att')
    #     train_match_01_min['01-check'+i_str] = []
    #     train_match_01_min['01-check'+i_str].append(min_matches)
    #     avg_matches = check_backend('1', str(i), train, be_path+filename, 'avg_att')
    #     train_match_01_avg['01-check'+i_str] = []
    #     train_match_01_avg['01-check'+i_str].append(avg_matches)

    #     min_matches = check_backend('1', str(i), train, be_path+filename, 'min_att')
    #     train_match_01_min['01-check'+i_str] = []
    #     train_match_01_min['01-check'+i_str].append(min_matches)
    #     avg_matches = check_backend('1', str(i), train, be_path+filename, 'avg_att')
    #     train_match_01_avg['01-check'+i_str] = []
    #     train_match_01_avg['01-check'+i_str].append(avg_matches)

    #     min_matches = check_backend('1', str(i), office, be_path+filename, 'min_att')
    #     office_match_01_min['01-check'+i_str] = []
    #     office_match_01_min['01-check'+i_str].append(min_matches)
    #     avg_matches = check_backend('1', str(i), office, be_path+filename, 'avg_att')
    #     office_match_01_avg['01-check'+i_str] = []
    #     office_match_01_avg['01-check'+i_str].append(avg_matches)

    #     min_matches = check_backend('1', str(i), queue, be_path+filename, 'min_att')
    #     queue_match_01_min['01-check'+i_str] = []
    #     queue_match_01_min['01-check'+i_str].append(min_matches)
    #     avg_matches = check_backend('1', str(i), queue, be_path+filename, 'avg_att')
    #     queue_match_01_avg['01-check'+i_str] = []
    #     queue_match_01_avg['01-check'+i_str].append(avg_matches)

    #     min_matches = check_backend('1', str(i), party, be_path+filename, 'min_att')
    #     party_match_01_min['01-check'+i_str] = []
    #     party_match_01_min['01-check'+i_str].append(min_matches)
    #     avg_matches = check_backend('1', str(i), party, be_path+filename, 'avg_att')
    #     party_match_01_avg['01-check'+i_str] = []
    #     party_match_01_avg['01-check'+i_str].append(avg_matches)

    #     min_matches = check_backend('1', str(i), movement, be_path+filename, 'min_att')
    #     movement_match_01_min['01-check'+i_str] = []
    #     movement_match_01_min['01-check'+i_str].append(min_matches)
    #     avg_matches = check_backend('1', str(i), movement, be_path+filename, 'avg_att')
    #     movement_match_01_avg['01-check'+i_str] = []
    #     movement_match_01_avg['01-check'+i_str].append(avg_matches)


    # NOTE: score scan instance
    # i = 0
    # for si in rx1c2:
    #     score = score_scan_instance(SWISS_COVID_THR1, SWISS_COVID_THR2,
    #             si, 'min_att', 'min_gtd', SWISS_COVID_CD)
    #     rx1c2[i]['min_sco'] = score
    #     i += 1


    # NOTE: Process backend information (collected through calibration app/GAEN v1.6)
    # jsons = get_en_jsons(be_path)
    # results_min = get_en_results(jsons, aggregation='min')
    # results_typ = get_en_results(jsons, aggregation='typical')
    # create_db('', 'gaen-min', True)
    # put_results('', results_min, 'gaen-min')
    # create_db('', 'gaen-typical', True)
    # put_results('', results_typ, 'gaen-typical')


    # NOTE: Process regular scenarios collected through packet logs
    create_db('', 'scenario01-lunch', rm_old=True)
    ti = (start_lunch, stop_lunch)
    for i in AND_HCI_ID:
        parse_hcicmds('pcaps/', i, ti)
    for i in AND_VS_ID:
        parse_vscmds('pcaps/', i, ti)
    for i in AND_ID:
        results = parse_hcievents('pcaps/', i, ti)
        put_results('', results, 'scenario01-lunch')
    results = get_sqlite_results('scenario01-lunch.sqlite', 'timestamp')
    lunch_json = get_cv_json(cv_path, '0_lunch.zip')
    check_cv_json_obj(lunch_json)
    results = add_gtd(results, lunch_json)
    create_db('', 'scenario01-lunch', rm_old=True)
    put_results('', results, 'scenario01-lunch')

    create_db('', 'scenario02-train', rm_old=True)
    ti = (start_train, stop_train)
    for i in AND_HCI_ID:
        parse_hcicmds('pcaps/', i, ti)
    for i in AND_VS_ID:
        parse_vscmds('pcaps/', i, ti)
    for i in AND_ID:
        results = parse_hcievents('pcaps/', i, ti)
        put_results('', results, 'scenario02-train')
    results = get_sqlite_results('scenario02-train.sqlite', 'timestamp')
    train_json = get_cv_json(cv_path, '1_train.zip')
    check_cv_json_obj(train_json)
    results = add_gtd(results, train_json)
    create_db('', 'scenario02-train', rm_old=True)
    put_results('', results, 'scenario02-train')

    create_db('', 'scenario03-office', rm_old=True)
    ti = (start_office, stop_office)
    for i in AND_HCI_ID:
        parse_hcicmds('pcaps/', i, ti)
    for i in AND_VS_ID:
        parse_vscmds('pcaps/', i, ti)
    for i in AND_ID:
        results = parse_hcievents('pcaps/', i, ti)
        put_results('', results, 'scenario03-office')
    results = get_sqlite_results('scenario03-office.sqlite', 'timestamp')
    coworking_json = get_cv_json(cv_path, '2_coworking.zip')
    check_cv_json_obj(coworking_json)
    results = add_gtd(results, coworking_json)
    create_db('', 'scenario03-office', rm_old=True)
    put_results('', results, 'scenario03-office')

    queue = []
    create_db('', 'scenario04-queue', rm_old=True)
    ti = (start_queue, stop_queue)
    for i in AND_HCI_ID:
        parse_hcicmds('pcaps/', i, ti)
    for i in AND_VS_ID:
        parse_vscmds('pcaps/', i, ti)
    for i in AND_ID:
        results = parse_hcievents('pcaps/', i, ti)
        put_results('', results, 'scenario04-queue')
    results = get_sqlite_results('scenario04-queue.sqlite', 'timestamp')
    queue_json = get_cv_json(cv_path, '3_queue.zip')
    check_cv_json_obj(queue_json)
    results = add_gtd(results, queue_json)
    create_db('', 'scenario04-queue', rm_old=True)
    put_results('', results, 'scenario04-queue')

    create_db('', 'scenario05-party', rm_old=True)
    ti = (start_party, stop_party)
    for i in AND_HCI_ID:
        parse_hcicmds('pcaps/', i, ti)
    for i in AND_VS_ID:
        parse_vscmds('pcaps/', i, ti)
    for i in AND_ID:
        results = parse_hcievents('pcaps/', i, ti)
        put_results('', results, 'scenario05-party')
    results = get_sqlite_results('scenario05-party.sqlite', 'timestamp')
    party_json = get_cv_json(cv_path, '4_party.zip')
    check_cv_json_obj(party_json)
    results = add_gtd(results, party_json)
    create_db('', 'scenario05-party', rm_old=True)
    put_results('', results, 'scenario05-party')

    create_db('', 'scenario06-movement', rm_old=True)
    ti = (start_movement, stop_movement)
    for i in AND_HCI_ID:
        parse_hcicmds('pcaps/', i, ti)
    for i in AND_VS_ID:
        parse_vscmds('pcaps/', i, ti)
    for i in AND_ID:
        results = parse_hcievents('pcaps/', i, ti)
        put_results('', results, 'scenario06-movement')
    results = get_sqlite_results('scenario06-movement.sqlite', 'timestamp')
    moving_json = get_cv_json(cv_path, '5_moving.zip')
    check_cv_json_obj(moving_json)
    results = add_gtd(results, moving_json)
    create_db('', 'scenario06-movement', rm_old=True)
    put_results('', results, 'scenario06-movement')

    # NOTE: generate json from sqlite
    ti = [ start_lunch.timestamp(), stop_lunch.timestamp()]
    lunch = gen_en_from_sqlite('scenario01-lunch.sqlite', ti, exp_win_mins)
    ti = [ start_train.timestamp(), stop_train.timestamp()]
    train = gen_en_from_sqlite('scenario02-train.sqlite', ti, exp_win_mins)
    ti = [ start_office.timestamp(), stop_office.timestamp()]
    office = gen_en_from_sqlite('scenario03-office.sqlite', ti, exp_win_mins)
    ti = [ start_queue.timestamp(), stop_queue.timestamp()]
    queue = gen_en_from_sqlite('scenario04-queue.sqlite', ti, exp_win_mins)
    ti = [ start_party.timestamp(), stop_party.timestamp()]
    party = gen_en_from_sqlite('scenario05-party.sqlite', ti, exp_win_mins)
    ti = [ start_movement.timestamp(), stop_movement.timestamp()]
    movement = gen_en_from_sqlite('scenario06-movement.sqlite', ti, exp_win_mins)
